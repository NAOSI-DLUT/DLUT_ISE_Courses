<!-- Created by Frank -->

- [机器学习\_概念\_3](#机器学习_概念_3)
  - [11 モデル推定](#11-モデル推定)
    - [モデル推定 (P5)](#モデル推定-p5)
    - [クラスタリング (P6)](#クラスタリング-p6)
      - [階層的⼿法 (P6)](#階層的法-p6)
      - [分割最適化⼿法 (P6)](#分割最適化法-p6)
    - [階層的クラスタリング (P7)](#階層的クラスタリング-p7)
    - [クラスタ間の類似度計算 (P10)](#クラスタ間の類似度計算-p10)
      - [単連結法](#単連結法)
      - [完全連結法](#完全連結法)
      - [重⼼法](#重法)
      - [Ward法](#ward法)
    - [分割最適化クラスタリング (P11)](#分割最適化クラスタリング-p11)
    - [k‐meansクラスタリング（k‐平均法）(P12)](#kmeansクラスタリングk平均法p12)
    - [X‐meansアルゴリズム (P16)](#xmeansアルゴリズム-p16)
    - [BIC (Bayesian Information Criterion, 贝叶斯信息准则) (P17)](#bic-bayesian-information-criterion-贝叶斯信息准则-p17)
    - [異常検出 (P18)](#異常検出-p18)
    - [外れ値の検出 (P18)](#外れ値の検出-p18)
      - [外れ値 (P18)](#外れ値-p18)
    - [局所異常因⼦ LOF (P19)](#局所異常因-lof-p19)
      - [周辺密度 (P19)](#周辺密度-p19)
      - [到達可能距離 (P20)](#到達可能距離-p20)
      - [局所到達可能密度 (P20)](#局所到達可能密度-p20)
      - [LOF(x) (P22)](#lofx-p22)
    - [確率密度推定 (P23)](#確率密度推定-p23)
    - [EMアルゴリズム Expectation‐Maximization (P25)](#emアルゴリズム-expectationmaximization-p25)
  - [12 Pattern Mining](#12-pattern-mining)
    - [Pattern Mining とは (P5)](#pattern-mining-とは-p5)
    - [Apriori 定理](#apriori-定理)
    - [Apriori Algorithm (P6)](#apriori-algorithm-p6)
    - [FP - Growth Algorithm (P18)](#fp---growth-algorithm-p18)
    - [推荐 system 中的学习 (P24)](#推荐-system-中的学习-p24)
  - [13 系列 Data 识别](#13-系列-data-识别)
    - [系列 Data (P5)](#系列-data-p5)
    - [系列 Data 识别问题分类 (P5)](#系列-data-识别问题分类-p5)
    - [1. 入力、出力等长 (系列标签问题, P9)](#1-入力出力等长-系列标签问题-p9)
      - [对数线性模型 (P11)](#对数线性模型-p11)
      - [CRF 条件付き確率場 (P14)](#crf-条件付き確率場-p14)
    - [2. 入力不定、出力长度为 1 (系列识别问题, P15)](#2-入力不定出力长度为-1-系列识别问题-p15)
      - [HMM 隐藏马尔科夫模型 (P17)](#hmm-隐藏马尔科夫模型-p17)
      - [系列识别问题的例子 (P15)](#系列识别问题的例子-p15)
    - [3. 入力出力之间没有明确关系](#3-入力出力之间没有明确关系)
  - [14 强化学习](#14-强化学习)
    - [强化学习 (P5)](#强化学习-p5)
    - [马尔科夫决定过程 (P6)](#马尔科夫决定过程-p6)
    - [1 状态问题的定式化 - K armed bandit (P7)](#1-状态问题的定式化---k-armed-bandit-p7)
      - [决定状况的定式化 (P8)](#决定状况的定式化-p8)
      - [非决定情况的定式化 (P9)](#非决定情况的定式化-p9)
    - [马尔科夫决定过程的定式化 (P10)](#马尔科夫决定过程的定式化-p10)
    - [Bellman 方程式 \& Q值的推导 (P14)](#bellman-方程式--q值的推导-p14)
    - [Q值的推定方法 (P15)](#q值的推定方法-p15)
      - [model base 手法 (P16)](#model-base-手法-p16)
      - [model free 手法 (P18)](#model-free-手法-p18)
      - [1. TD 时间差分学习 (P18)](#1-td-时间差分学习-p18)
      - [2. 决定的 TD 学习 (P19)](#2-决定的-td-学习-p19)
      - [3. 概率的 TD 学习 (P22)](#3-概率的-td-学习-p22)
  - [15 半教師あり学習](#15-半教師あり学習)
    - [半教師あり学習 (P4)](#半教師あり学習-p4)
    - [1. 数值特征的情况 (P6)](#1-数值特征的情况-p6)
      - [适合的数据 (P6)](#适合的数据-p6)
      - [不适合的数据 (P7)](#不适合的数据-p7)
      - [半教師あり学習が可能な data の仮定 (P8)](#半教師あり学習が可能な-data-の仮定-p8)
    - [2. category 特征的情况 (P9)](#2-category-特征的情况-p9)
      - [适合的数据 (P9)](#适合的数据-p9)
      - [Overlap 的传播 (P10)](#overlap-的传播-p10)
    - [半教師あり学習 Algorithm (P11)](#半教師あり学習-algorithm-p11)
    - [自己学习 Self-Training (P13)](#自己学习-self-training-p13)
    - [共训练 Co-Training (P14)](#共训练-co-training-p14)
    - [YATSI Algorithm Yet Another Two-Stage Idea (P16)](#yatsi-algorithm-yet-another-two-stage-idea-p16)
    - [Label 传播法 (P18)](#label-传播法-p18)

# 机器学习_概念_3

## <font color="569cd6">11 モデル推定</font>

### モデル推定 (P5)

* 特徴ベクトルの要素が数値である場合に、その特徴ベクトルが⽣じるもとになったクラスを推定すること
* 特徴空間上にあるデータのまとまりを⾒つける問題

### クラスタリング (P6)

* 与えられたデータをまとまりに分ける操作

* 内的結合と外的分離が達成されるような部分集合に分割すること
  
  * 内的結合: ⼀つのまとまりを認められるデータ、なるべく近く
  
  * 外的分離: 異なったまとまり間の距離は、なるべく遠く
    
    #### 階層的⼿法 (P6)

* 個々のデータをボトムアップ (bottom up) 的にまとめる
  
  #### 分割最適化⼿法 (P6)

* 全体のデータをトップダウン (top down) 的に分割する

### 階層的クラスタリング (P7)

* 近くのデータをまとめて⼩さいクラスを作り
* クラスタを融合する操作を⽊構造で記録
* 処理を途中でやめる、任意のクラスタ数からなるクラスタリングが得られる

### クラスタ間の類似度計算 (P10)

#### 単連結法

* 最も近いデータ対

* ⼀⽅向に伸びやすくなる傾向
  
  #### 完全連結法

* 最も遠いデータ対

* ⼀⽅向に伸びるのを避ける傾向
  
  #### 重⼼法

* クラスタの重⼼間の距離
  
  #### Ward法

* 「クラスタ中⼼との距 離の⼆乗和」を求め、融合後から融合前を引いたもの

* 階層的クラスタリングでよく⽤いられる

### 分割最適化クラスタリング (P11)

* 全体的な視点でまとまりの良いクラスタを求める⽅法
* 評価する関数を定め、評価関数の値を最適化

### k‐meansクラスタリング（k‐平均法）(P12)

* 評価関数を「各データと所属するクラスタ中⼼との距離の総和」と定義
* 问题点 (P16)
  * 事前にクラスタ数 を決めないといけない

### X‐meansアルゴリズム (P16)

* クラスタ数を適応的に決定する⽅法

### BIC (Bayesian Information Criterion, 贝叶斯信息准则) (P17)

* BICが⼩さいと良い

### 異常検出 (P18)

* 異常値を、教師なし信号で⾒つける

### 外れ値の検出 (P18)

* 最も基礎的な異常検出
  
  #### 外れ値 (P18)

* 他と⼤きく異なるデータ
  
  * 極端に離れたデータ
  * 他のクラスのデータに紛れ込んだデータ

### 局所異常因⼦ LOF (P19)

* 近くにデータが無い（あるいは極端に少ない）ものを外れ値とみなす⽅法
  
  #### 周辺密度 (P19)

* 周辺（𝑘番⽬までに近いデータがある範囲）にあるデータまでの距離の平均
  
  #### 到達可能距離 (P20)
  
  #### 局所到達可能密度 (P20)

* 到達可能距離を⽤いて定義された 𝒙の周辺密度
  
  #### LOF(x) (P22)

* LOF(x) が 1 に近い値: 正常なデータ

* LOF(x) が⼤きな値: 外れ値

### 確率密度推定 (P23)

* 教師なし学習で識別器を作ることを考え
* 事後確率が最⼤となる識別器を作る
* 各クラスタの確率分布の形を仮定して、そのパラメータを学習データから推定する問題を設定

### EMアルゴリズム Expectation‐Maximization (P25)

* Eステップ と Mステップを順に繰り返してパラメータの最尤推定量を得る
* Eステップ
  * クラスタに属する確率を計算
* Mステップ
  * Eステップでの確率をデータの重みとして、パラメータを再計算

## <font color="569cd6">12 Pattern Mining</font>

### Pattern Mining とは (P5)

* 頻出項目抽出: pattern を抽出
* 連想規則抽出: 根据 pattern 查找规则

### Apriori 定理

* 频繁集 (P8): 项集的支持度超过设定的阈值
* 対偶 (P9): 逆否命题
* 如果一个集合是频繁集，它的子集也是频繁集；如果一个集合不是频繁集，它的超集也不是频繁集 (P10)
* 如果某个结论规则的置信度高的话，它的子集的置信度也高；如果某个结论规则的置信度低的话，他的超集的置信度也低 (P15)

### Apriori Algorithm (P6)

* 利用 Apriori 定理的逆否命题，从小项目集合开始计算支持度，不扩展不频繁出现的集合，减少要调查的项目集合

* 频出项目抽出 (P6)
  
  * transaction (P6): 一条购买记录构成一个 transaction，也就是一条数据
  * 支持度 (P6)

* 连想规则抽出 (P12)
  
  * 置信度 (P13)
  * lift 值 (P13)
  * 作成手顺: (P12)
    * 抽出频繁集
    * 集合分成条件部、结论部，生成可能的规则集合
    * 评估规则的有用性，并缩小可能有用的范围

### FP - Growth Algorithm (P18)

* 高速化 Apriori 算法
* 将 transaction data 转换为 `compact情報` (紧凑信息) 并对该信息进行 pattern mining
* compact 手順 (P18)
* FP - Growth Algorithm の例 (P19 - 22)
* 对 FP 木进行 pattern mining (P23)

### 推荐 system 中的学习 (P24)

* 协调 filtering
* Matrix Factorization
  * Alternating Least Squares
  * Non-negative Matrix Factorization

## <font color="569cd6">13 系列 Data 识别</font>

### 系列 Data (P5)

各个要素之间相互独立，无依赖关系

### 系列 Data 识别问题分类 (P5)

### 1. 入力、出力等长 (系列标签问题, P9)

* 例: 形态素解析，一个词语对应一个品词
* point: 输入输出前后存在依赖关系 (P6)
* 系列数据一个一个进行识别の問題点
  * 1: 系列の性質を捨ててしまう
  * 2: 膨大な class 数

#### 对数线性模型 (P11)

* 将前面和后面的输入输出自由组合为特征向量的元素，反映一个序列
* 可以解决问题1

#### CRF 条件付き確率場 (P14)

* 输出序列只参照前面一个元素，而输入序列自由范围参照
* 遷移素性 (P13): 出力系列参照的素性
* 観測素性 (P13): 入力系列对应的素性
* ビタビ Algorithm (P14): 求最大值

### 2. 入力不定、出力长度为 1 (系列识别问题, P15)

* 例: 识别出特定的分类
* point: (P7)
  * 输入数据长度不定
  * 输入数据有共同性质，但是如何分割是不确定的
  * 教師なし学習と教師あり学習を組み合わせ

#### HMM 隐藏马尔科夫模型 (P17)

* 遷移確率 (P17): 状态 i 到状态 j 的转移概率 a<sub>ij</sub>
* 出力確率 (P17): 在状态 i 时的输出 o 的概率 b<sub>i</sub>(o)

#### 系列识别问题的例子 (P15)

通过判断用户的 Key 和 mouse 的输入操作，识别是新手还是熟练

* EM 算法，不断计算 P(**x**) 和 HMM 的参数 a<sub>ij</sub>, b<sub>i</sub>(o) (P18)
* 通过 ビタビ Algorithm ，求得相对于输入序列，概率最大的转移序列 (P19)
* 最大概率的转移序列确定，全部隐藏变量的位置也就确定，每个输入的子序列的输出也是确定的 (P19)

### 3. 入力出力之间没有明确关系

## <font color="569cd6">14 强化学习</font>

### 强化学习 (P5)

* 为了获得报酬，对环境进行执行操作的 `意思決定 agent` 的学习
* 意思决定 agent: 决策代理，如 robot、象棋运行的程序
* agent 以获得更多的报酬为目的 (是个函数)
  * 输入: 状态或状态的概率分布
  * 输出: 行为
* 利用 马尔科夫决定过程 (MDP)

### 马尔科夫决定过程 (P6)

假定了以下的条件

* 环境是离散的状态集合
* 在时刻t，状态是 `st`，agent 执行了行为 `at` 后，获得报酬 `r(t+1)`，状态迁移至 `s(t+1)`
* 状态迁移是概率性的，该概率只依赖于迁移前的状态

### 1 状态问题的定式化 - K armed bandit (P7)

马尔科夫决定过程里最简单的例子

* 1 状态: 只有一台 slot machine
* K armed: 有多个 arm 把手，也就是有 K 个选择
* 报酬: 立即给予

#### 决定状况的定式化 (P8)

* 所有的情况一一进行尝试，选择报酬最高的作为学习结果

#### 非决定情况的定式化 (P9)

* 行为a对应的报酬r是服从概率 p(r|a)的
* 多次尝试，选取平均报酬多的作为结果

### 马尔科夫决定过程的定式化 (P10)

* 报酬和迁移到下一个状态的概率，只与现在的状态和行为有关
* 评价一个策略 pi 的好坏，根据策略执行时累积的报酬期待值 `V(st)` 进行评价 (P11)
* 学习的目标，就是获得最优的策略 pi (P12)
* 最优策略: 对于所有状态，累积报酬的期待值达到最大的策略 (P12)

### Bellman 方程式 & Q值的推导 (P14)

### Q值的推定方法 (P15)

#### model base 手法 (P16)

环境 model 化，给定了 `状态迁移概率` 和 `报酬的概率分布`。考虑使用动态规划法

* 通过 `value iteration Algorithm`，求出状态评价函数V(s)的最优值（Q值最大）
* `value iteration Algorithm` (P17)
  * 第 1 回，获得 goal 前一个状态的值
  * 第 2 回，获得 goal 前两个状态的值，同时第 1 回的值还要更新

#### model free 手法 (P18)

状态迁移概率和报酬的概率分布未知，`試行錯誤` を通じて環境と相互作用をした結果を使って学習する (P3)

#### 1. TD 时间差分学习 (P18)

* 将 `e - greedy` 法作为探索策略
  
  * 最适行为: 概率 1-e
  
  * 其他行为: 概率 e
  
  * 实际上通过以下式子进行行动的选择 (P18)
    
    #### 2. 决定的 TD 学习 (P19)
    
    报酬和迁移未知，但是已经决定 (?)

* 此时，在 bellman 方程式中，将概率去除

* 例子: P21
  
  #### 3. 概率的 TD 学习 (P22)
  
  Q值按照一定的比例更新，这个比例随着时间而减少

* P22

## <font color="569cd6">15 半教師あり学習</font>

### 半教師あり学習 (P4)

* 有数值形式、category 形式，教師あり、教師なし data 混杂在一起
* 通过有正确答案的数据构建判别器，没有正确答案的用来提高判别器的性能
* 实际上是少量的有正确答案的数据、大量的没有正确答案的数据

### 1. 数值特征的情况 (P6)

#### 适合的数据 (P6)

* 没有混杂，界限分明

* 可以通过有正确答案的分布，按比例准确估计没有正确答案数据的信息

* 识别器性能可能向上
  
  #### 不适合的数据 (P7)

* 不能明确分类

* 识别边界的位置存在很大的不同

* 识别器的性能可能下降
  
  #### 半教師あり学習が可能な data の仮定 (P8)

* 平滑性假定
  
  * 输入的 x1, x2 在高密度区域的话，输出的 y1, y2 有关联

* cluster 假定

* 低密度分离
  
  * 识别分界在低密度区域

* 多样性假定

### 2. category 特征的情况 (P9)

* 类别特征的学习数据能够大量输入的问题，基本上都是输入语言数据
  
  #### 适合的数据 (P9)

* 有正确答案的数据可以抽出很多特征语，且没有正确答案的数据也都包含这些特征语

* 对识别有用的特征语的 overlap(重叠) 很多的数据、即使没有正解，也可以高精度预测
  
  #### Overlap 的传播 (P10)

* 有正解数据的特征语和 overlap 很多。无正解数据会变为新的有正解数据

* 新的有正解数据的特征语加入到特征语集合中

* 半教師あり学習比较适合「文書分類問題」

### 半教師あり学習 Algorithm (P11)

* 基本思想: 用无正解的数据，去调整通过有正解数据生成的识别器的参数

### 自己学习 Self-Training (P13)

* 在用有正解数据做出来的识别器的输出中，相信高可信度的结果，并重复将数据纳入有正解数据的集合中，重新训练自己
* 不满足低密度分离的数据，会影响识别器 (P13)

### 共训练 Co-Training (P14)

* 创建两个具有不同特征的分类器，并利用彼此的识别结果来训练每个分类器
* 两个具有不同判断标准的分类器相互学习
* 优点
  * 在初期对错误有很强的抵抗力
* 缺点
  * 不一样的特征集合很难找
  * 自己学习可能性能更好
* 共训练 Algorithm (P15)

### YATSI Algorithm Yet Another Two-Stage Idea (P16)

* 识别器只学习有正解数据一次，然后识别所有无正解数据，结果结合权重，通过 k-NN 法
* 旨在避免由于迭代算法中的重复而导致的错误放大
* 自己学习和共训练基本用于而分类问题，而 YATSI 适用于多分类问题
* YATSI Algorithm (P17)

### Label 传播法 (P18)

* data 看作是 node，基于类似度，构造 graph 结构
* 假设附近的节点可能属于同一类，预测无正解数据
* 主要是将评价函数 J(**f**) 最小化 (P18)
* 学習手順 (P19)
  * 1. 基于数据间的类似度，构建 graph
    * 标准
      * Gauss kernel: 所有节点都被连接，并赋予一个连续的结合值
      * k-NN法: 只连接附近 k 个节点
  * 2. 最小化评价函数
    * 重复进行 ”从有正解 node 向无正解 node 传播 label“ 的操作，通过最小化评价函数，进行优化使得邻接 node 具有相同的 label 